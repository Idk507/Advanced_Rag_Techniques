{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "import os \n",
    "import chromadb \n",
    "from langchain.vectorstores import Chroma \n",
    "from langchain.document_transformers import LongContextReorder \n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings \n",
    "from langchain.retrievers.merger_retriever import MergerRetriever \n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import  RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\miniconda3\\envs\\idk\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"BAAI/bge-large-en\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "loader_un_sdg = PyPDFLoader(\"data/UN SDG.pdf\")\n",
    "documents_un_sdg = loader_un_sdg.load()\n",
    "text_splitter_un_sdg = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)\n",
    "text_un_sdg = text_splitter_un_sdg.split_documents(documents_un_sdg)\n",
    "\n",
    "loader_paris_agreement = PyPDFLoader(\"data/english_paris_agreement.pdf\")\n",
    "documents_paris_agreement = loader_paris_agreement.load()\n",
    "text_splitter_paris_agreement = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
    "                                                   chunk_overlap=100)\n",
    "texts_paris_agreement = text_splitter_paris_agreement.split_documents(documents_paris_agreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_sdg_store = Chroma.from_documents(text_un_sdg, hf, collection_metadata={\"hnsw:space\": \"cosine\"}, persist_directory=\"store/un_sdg_chroma_cosine\")\n",
    "\n",
    "paris_agreement_store = Chroma.from_documents(texts_paris_agreement, hf, collection_metadata={\"hnsw:space\": \"cosine\"}, persist_directory=\"store/paris_chroma_cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_un_sdg_store = Chroma(persist_directory=\"store/un_sdg_chroma_cosine\", embedding_function=hf)\n",
    "print(\"First Vector Store Loaded.........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_paris_agreement_store = Chroma(persist_directory=\"store/paris_chroma_cosine\", embedding_function=hf)\n",
    "print(\"Second Vector Store Loaded........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_un_sdg = load_un_sdg_store.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\":3})\n",
    "\n",
    "retriever_paris_agreement = load_paris_agreement_store.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lotr = MergerRetriever(retrievers=[retriever_un_sdg, retriever_paris_agreement])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for chunks in lotr.get_relevant_documents(\"Is there any framework available to tackle the climate change?\"):\n",
    "    print(chunks.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Is there any framework available to tackle the climate change?\"\n",
    "docs = lotr.get_relevant_documents(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "See this result is too much messy now lets refine it according to the question and overcome the situation of lost in middle\n",
    "Now After understanding step by step it create a pipeline for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_transformers import (\n",
    "    EmbeddingsClusteringFilter,\n",
    "    EmbeddingsRedundantFilter,\n",
    ")\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.document_transformers import LongContextReorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import search\n",
    "filter = EmbeddingsRedundantFilter(embeddings=hf)\n",
    "reordering = LongContextReorder()\n",
    "pipeline = DocumentCompressorPipeline(transformers=[filter, reordering])\n",
    "compression_retriever_reordered = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline, base_retriever=lotr,search_kwargs={\"k\": 3, \"include_metadata\": True}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "llms = LlamaCpp(streaming=True,\n",
    "                   model_path=\"/content/drive/MyDrive/zephyr-7b-beta.Q4_K_M.gguf\",\n",
    "                   max_tokens = 1500,\n",
    "                   temperature=0.75,\n",
    "                   top_p=1,\n",
    "                   gpu_layers=0,\n",
    "                   stream=True,\n",
    "                   verbose=True,n_threads = int(os.cpu_count()/2),\n",
    "                   n_ctx=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "     \n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "      llm=llms,\n",
    "      chain_type=\"stuff\",\n",
    "      retriever = compression_retriever_reordered,\n",
    "      return_source_documents = True\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query =\"who is jon snow?\"\n",
    "results = qa(query)\n",
    "print(results['result'])\n",
    "#\n",
    "print(results[\"source_documents\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
